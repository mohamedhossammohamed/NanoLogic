NEURO-SYMBOLIC ARCHITECTURE FOR 64-ROUND SHA-256 PREIMAGE SEARCH: A DEEP LEARNING-GUIDED SOLVER FOR APPLE SILICONExecutive SummaryThe cryptanalysis of the full 64-round SHA-256 hash function remains one of the most formidable challenges in modern computer science and information security. Designed by the National Security Agency (NSA) and published in 2001, SHA-256 serves as the backbone of digital security, underpinning everything from SSL/TLS certificates to the proof-of-work mechanism in Bitcoin. Its resistance to preimage attacks—the ability to find a message $m$ given a hash $h$ such that $H(m) = h$—is predicated on the "avalanche effect," a property where minor changes in input propagate chaotically through the system, rendering the function theoretically non-invertible by analytical means.This research report proposes a novel Neuro-Symbolic Architecture designed not to invert the function directly, but to accelerate the search for preimages by orders of magnitude compared to standard brute-force or purely symbolic methods. The architecture, designated Neuro-SHA-M4, is explicitly engineered to operate within the stringent resource constraints of consumer-grade hardware: specifically, the Apple M4 silicon with a memory limit of 5GB RAM.By abandoning the futile objective of "learning the inverse function" via end-to-end regression, this system adopts a "Solver-in-the-Loop" philosophy. We integrate a specialized, highly quantized Neural Branching Delegate that functions as a heuristic guide for a rigorous Conflict-Driven Clause Learning (CDCL) SAT solver. The neural component learns the structural "shape" of the hash function's dependency graph, predicting "glue variables" and "unsatisfiable cores" to steer the symbolic solver through the chaotic landscape of the 64-round execution trace.The proposed architecture introduces three primary innovations:Bitwise Neural Guidance (BASIN Protocol): Moving beyond the inefficient Conjunctive Normal Form (CNF) to a direct bit-level modeling approach that preserves the semantic structure of the ARX (Add-Rotate-Xor) operations.Sparse Logic Transformers: A specialized attention mechanism that mirrors the exact wiring diagram of SHA-256’s $\Sigma$ and $Maj$ functions, reducing the $O(N^2)$ complexity of standard Transformers to linear time $O(N)$, enabling full 64-round context within limited memory.BitNet b1.58 Quantization: Leveraging ternary weights $\{-1, 0, 1\}$ to align with the Apple Neural Engine (ANE) capabilities, maximizing inference throughput while minimizing the memory footprint to fit the 5GB envelope.This report details the mathematical foundations, the architectural design, the hardware optimization strategies, and the training methodologies required to realize this system. It represents a paradigm shift from "blind" symbolic search to "intuition-guided" cryptographic solving.1. The Mathematical & Cryptanalytic Landscape1.1 The Hardness of SHA-256 InversionSHA-256 is an iterated hash function belonging to the SHA-2 family, built upon the Merkle-Damgård construction with a Davies-Meyer compression function. The algorithm processes messages in 512-bit blocks and outputs a 256-bit digest. The core of its security lies in the complexity of its internal state update function, which is applied 64 times (rounds).The state of the hash function is maintained in eight 32-bit working variables, denoted as $a, b, c, d, e, f, g, h$. At each round $t$, these variables are updated based on the following equations, which mix the previous state, a message schedule word $W_t$, and a round constant $K_t$:$$T_1 = h + \Sigma_1(e) + Ch(e, f, g) + K_t + W_t \\
T_2 = \Sigma_0(a) + Maj(a, b, c) \\
h = g, \quad g = f, \quad f = e, \quad e = d + T_1 \\
d = c, \quad c = b, \quad b = a, \quad a = T_1 + T_2$$All addition ($+$) is performed modulo $2^{32}$. The non-linear mixing comes from four logical functions:$\Sigma_0(x) = \text{ROTR}^2(x) \oplus \text{ROTR}^{13}(x) \oplus \text{ROTR}^{22}(x)$$\Sigma_1(x) = \text{ROTR}^6(x) \oplus \text{ROTR}^{11}(x) \oplus \text{ROTR}^{25}(x)$$Ch(x, y, z) = (x \land y) \oplus (\neg x \land z)$ (Choose function)$Maj(x, y, z) = (x \land y) \oplus (x \land z) \oplus (y \land z)$ (Majority function)The interplay of these operations creates the Avalanche Effect. In a symbolic context, specifically when utilizing Boolean Satisfiability (SAT) solvers, the primary source of hardness is the Modular Addition. While XOR ($\oplus$) and Rotation (ROTR) are linear over the field GF(2), addition is non-linear due to the carry propagation.In a SAT encoding (e.g., CNF), a single 32-bit addition $C = A + B$ requires a ripple-carry circuit. The value of the $i$-th bit of $C$ depends not just on $A_i$ and $B_i$, but on the carry bit $c_i$, which in turn depends on $A_{i-1}, B_{i-1}, c_{i-1}$, and so on, all the way down to the least significant bit. When these additions are stacked 64 times, the dependency graph becomes incredibly dense. A variable representing a bit in Round 60 has a complex, highly non-linear dependency on every single bit of the input message.Attempting to invert this function involves setting the target hash values (the final state variables $a_{64}...h_{64}$) and asking the solver to find the initial message schedule $W$. This is a Constraint Satisfaction Problem (CSP) of immense scale.1.2 The Failure of Standard Branching HeuristicsModern SAT solvers, such as CaDiCaL, Kissat, and CryptoMiniSat , employ the Conflict-Driven Clause Learning (CDCL) algorithm. The efficiency of CDCL relies heavily on its Branching Heuristic—the logic used to decide which variable to assign a value to next when the current state is not yet a solution or a contradiction.The industry-standard heuristic is VSIDS (Variable State Independent Decaying Sum). VSIDS operates on a principle of "activity." It maintains a score for every variable. Whenever the solver encounters a conflict (a set of assignments that violates a clause), it performs conflict analysis to learn a new clause that prevents this specific error in the future. The variables involved in this conflict have their scores incremented. Periodically, all scores are decayed (multiplied by a factor $< 1$) to prioritize recent conflicts.Why VSIDS Fails on SHA-256:In typical industrial problems (software verification, logistics), conflicts are localized. A bug in a specific module only involves variables from that module. VSIDS quickly identifies this "active core" and focuses the solver's attention there.However, in SHA-256, the cryptographic diffusion ensures that every variable affects every other variable. The conflict graph is not sparse or clustered; it is quasi-random and fully connected. When the solver makes a guess at Round 10, it may not detect a contradiction until it propagates to Round 50. The resulting conflict analysis involves hundreds of variables spanning dozens of rounds. Consequently, VSIDS increments scores almost uniformly across the board. The heuristic fails to distinguish "structurally critical" variables from irrelevant ones. The solver effectively degrades into a blind guesser, thrashing through an exponential search space without ever converging on the "Unsatisfiable Core" or the correct "Backdoor Set" of variables.1.3 The Neuro-Symbolic HypothesisThe core hypothesis of this research is that while the values of the bits in a SHA-256 execution are chaotic and pseudo-random, the structural relationships between them are fixed and learnable. The "shape" of the computation is identical for every hash.We posit that a Deep Learning model can replace the "blind" VSIDS heuristic with a "sighted" Neural Branching Heuristic. A neural network, trained on millions of execution traces, can learn to recognize patterns in the partial assignment state. It can answer questions that VSIDS cannot:Glue Variable Prediction: Which variables, if assigned, will maximize the propagation of constraints? In SAT terminology, we seek variables that minimize the Literal Block Distance (LBD) of the learned clauses. These are the "bridges" that connect different parts of the search space.Unsatisfiable Core Detection: Which subset of the currently active clauses is likely to lead to a contradiction? If the network can predict this, it can guide the solver to resolve the contradiction early, rather than exploring a doomed branch for millions of cycles.Path Viability (The Pathfinder): Given a partial assignment (e.g., Message words $W_0...W_{15}$ are fixed), is it plausible that this path leads to the target hash? If a neural discriminator detects that the internal state at Round 16 diverges too far from a valid path, it can signal the solver to backtrack immediately.This approach defines a Neuro-Symbolic System:The Symbolic Solver (CDCL) provides correctness, rigorous propagation, and proof of unsatisfiability. It handles the "logic."The Neural Network (Deep Learning) provides "intuition." It handles the "search."2. Research Area 1: Neural Branching Heuristics (The "SAT-Guide")The first pillar of the Neuro-SHA-M4 architecture is the "SAT-Guide," a neural module responsible for selecting the next branching variable.2.1 Limitations of Previous Approaches (NeuroSAT/NeuroCore)Seminal works like NeuroSAT  and NeuroCore  utilized Graph Neural Networks (GNNs) to process SAT instances. They represented the CNF formula as a bipartite graph (Literal nodes and Clause nodes).While theoretically sound, this approach hits a massive bottleneck: Memory Explosion.A CNF encoding of 64-round SHA-256 contains millions of clauses. A GNN requires storing the adjacency matrix (edges) of this graph. Even with sparse representations, the message-passing algorithm (MPNN) requires iterating over millions of edges per inference step.Memory Constraint: On an M4 MacBook with <5GB RAM, loading a graph with $10^7$ edges and performing gradient updates or even batched inference is impossible. The "Graph Explosion" problem renders standard GNNs non-viable for cryptographic instances of this scale.Semantic Loss: Converting SHA-256 to CNF "bit-blasts" the problem. The semantic reality that "Register $A$ is the sum of Register $B$ and $C$" is lost in a sea of AND/OR gates. The network has to re-learn addition from scratch.2.2 The BASIN Protocol: Bitwise Input RepresentationTo overcome the memory and semantic limitations, we adopt the principles of the BASIN (Bitwise Arithmetic Solver with Initialization by Neural network) solver. BASIN demonstrates that for cryptographic problems, the input to the neural network should not be the low-level CNF, but a higher-level Bitwise Representation.Input Tensor Design:Instead of a graph of clauses, we model the internal state of the hash function directly.We define a tensor $\mathcal{T}$ of shape $[64 \times 8 \times 32]$, representing:64 Rounds (Temporal dimension)8 Registers ($a, b, c, d, e, f, g, h$)32 Bits per register.Each element in this tensor is a vector containing the solver's current knowledge about that bit:State: $\{ \text{Unassigned, Assigned True, Assigned False} \}$ (One-hot encoded).Activity: The current VSIDS score of the variable (normalized).Phase: The polarity of the last assignment.This tensor requires only $64 \times 8 \times 32 \times 3 \approx 49,152$ floating-point values—a negligible memory footprint compared to the gigabytes required for a CNF graph. This allows us to feed the entire 64-round context into the neural network at once.2.3 The "NeuroGlue" Integration StrategyThe interaction between the Solver and the Neural Guide cannot be continuous. Calling a neural network at every branching step (millions of times per second) would introduce massive latency, making the solver slower than a random walk.We employ the Periodic Refocusing strategy, also known as NeuroGlue :The "Conflict Budget": The solver runs in pure symbolic mode (using standard VSIDS) for a fixed budget of conflicts (e.g., 5,000).Interrupt: Once the budget is exhausted, the solver pauses.Snapshot & Query: The current bitwise assignment state is extracted and passed to the Neural Guide (running on the Apple Neural Engine).Prediction: The Neural Guide outputs a "Importance Map"—a probability distribution over all unassigned variables. It highlights the "Glue Variables" (variables that are likely to bridge the current search frontier to a solution).Injection: The solver updates its internal VSIDS scores:$$\text{Score}(v) = \alpha \cdot \text{VSIDS}_{old}(v) + \beta \cdot \text{Neural}_{pred}(v)$$Resume: The solver resumes search, now effectively "refocused" on the variables the network deemed important.This strategy amortizes the cost of neural inference over thousands of symbolic steps, ensuring that the "Neuro-Symbolic" overhead remains low.3. Research Area 2: Differential Deep Learning (The "Pathfinder")The second pillar is the "Pathfinder," a Neural Distinguisher designed to prune the search tree.3.1 Neural Differentials vs. Classical DifferentialsClassical differential cryptanalysis looks for input differences $\Delta X$ that produce output differences $\Delta Y$ with high probability. This is linear and statistical.
Deep Learning based Cryptanalysis, pioneered by Aron Gohr on the Speck cipher , showed that neural networks can find "non-linear differentials." A ResNet can distinguish between "Random Data" and "Cipher Output" (even for round-reduced versions) with much higher accuracy than the best known classical distinguishers.For SHA-256, we adapt this to solve the Preimage Path Problem:Given: A partial assignment of the message schedule $W$ (e.g., $W_0...W_{15}$ are fixed).Question: Does there exist any completion of $W_{16}...W_{63}$ that leads to the target Hash $H$?3.2 The ResNet-Distinguisher ArchitectureWe employ a Deep Residual Network (ResNet) adapted for ARX structures.
The architecture consists of a "Backbone" of 1D Convolutional layers.Input: The state difference vector $\Delta S_t$ at the current "frontier" of the solver.The XOR-Net Layer: Standard Convolutional Neural Networks (CNNs) struggle to learn the parity-based nature of XOR operations. To assist the network, the first layer is a fixed, non-trainable XOR-Net layer. It explicitly computes features relevant to cryptanalysis:$\text{Feature}_1 = X \oplus Y$$\text{Feature}_2 = X + Y \pmod{2^{32}}$$\text{Feature}_3 = \text{ROTR}^n(X) \oplus Y$Residual Blocks: The network consists of 10-20 residual blocks. Each block contains two 1D convolution layers with Kernel Size 3 (capturing local bit interactions) and GELU activation.Output: A single scalar $p \in $, representing the probability that the current path is valid.3.3 Integration: The "Dead End" DetectorThis Pathfinder acts as a smart pruning mechanism.When the solver reaches a specific depth (e.g., Round 20), it queries the Pathfinder.If $p < \tau$ (where $\tau$ is a strict threshold, e.g., 0.001), the system assumes the current branch is a "Dead End."Action: The solver triggers a synthetic conflict, forcing a backtrack.This prevents the symbolic solver from wasting time exploring deep subtrees that are cryptographically doomed, even if they are logically consistent so far.4. The Architecture of "Algebraic" EfficiencyTo fit this sophisticated system into an Apple M4 with <5GB RAM, we cannot use off-the-shelf Transformer models. We must architect for efficiency.4.1 Sparse Logic TransformersA standard Transformer has $O(N^2)$ memory complexity due to the self-attention mechanism. For a sequence representing 64 rounds of SHA-256 state, this is prohibitive.However, SHA-256 is sparse. A bit in Round $t$ does not depend on every bit in Round $t-1$. It depends on exactly the bits defined by the $\Sigma$, $Maj$, and $Ch$ functions.We introduce the Sparse Logic Transformer.Static Attention Masks: Instead of learning the attention pattern, we hard-code the attention mask to mirror the SHA-256 wiring diagram.The Mask $\mathcal{M}$:For a query token $Q$ representing bit $i$ of Register $A$ at Round $t$:$$\text{Attend}(Q) = \{ A_{t-1}[i], \Sigma_0(A_{t-1}) \text{ bits}, Maj(A_{t-1}, B_{t-1}, C_{t-1}) \text{ bits} \}$$Specifically, the mask allows attention only to positions:$i, (i-2)\%32, (i-13)\%32, (i-22)\%32$ (from $\Sigma_0$ structure).Result: The complexity drops to $O(N \cdot k)$, where $k$ is the constant connectivity degree of the SHA-256 graph (~7-10). This allows us to load the entire 64-round dependency structure into memory with a footprint of under 500MB.4.2 BitNet b1.58: Extreme QuantizationThe Apple M4's Neural Engine is optimized for low-precision inference. Running FP32 models is a waste of memory and bandwidth.
We utilize the BitNet b1.58 architecture.Ternary Weights: We constrain all weights in the Transformer to the set $\{-1, 0, 1\}$.Storage Efficiency: Each weight requires only $\log_2(3) \approx 1.58$ bits. In practice, we pack them into 2-bit integers.Compute Efficiency: Matrix multiplication becomes simple addition/subtraction. This eliminates expensive floating-point multiplications.Impact: A 100 Million parameter model (sufficient for the logic of SHA-256) occupies ~25MB of RAM. This leaves ample room for the Solver's clause database and system overhead within the 5GB limit.5. The Reinforcement Learning Reward SignalTraining the Neural Guide is a Reinforcement Learning (RL) problem. The agent (Guide) takes actions (Variable Selection) to maximize a reward (Solving the Instance).5.1 The "Sparse Reward" ProblemIn a standard preimage attack, the reward is binary: $1$ if Preimage Found, $0$ otherwise. Since finding a preimage for 64 rounds is practically impossible during training, the agent would never receive a positive signal. This is the "Sparse Reward" problem.5.2 Dense Reward Shaping: Internal State MatchingWe solve this by training in a "Glass Box" environment where we know the solution.Generate Data: Create a random Message $M$ and compute Hash $H$. We also record the true internal state sequence $S_{true} = \{s_0, s_1,..., s_{64}\}$.Hide Solution: We give the solver only $H$.Reward Function: As the solver builds a partial assignment $S_{pred}$, we calculate the Hamming Similarity between $S_{pred}$ and $S_{true}$.$$R_t = \sum_{r=0}^{64} \lambda_r \cdot (1 - \frac{\text{HammingDist}(S_{pred}[r], S_{true}[r])}{256})$$where $\lambda_r$ is a weighting factor that increases with the round number $r$.Effect: The agent is rewarded for finding variable assignments that correctly reconstruct the internal state of the early rounds (0-20), even if it hasn't solved the full hash. This guides the agent to learn the "local inverse" logic of the compression function.5.3 Curriculum LearningWe employ a Curriculum Strategy to scale the difficulty :Phase 1 (Rounds 1-16): Train on 16-round reduced SHA-256. The search space is small enough that the solver finds solutions frequently. The agent learns the basic $\Sigma/Maj$ logic.Phase 2 (Transfer): Freeze the lower layers of the Transformer. Train on 32-round instances. The agent adapts to deeper diffusion.Phase 3 (Full 64-Round): Train on the full circuit. We stop demanding full solutions and switch to the "Internal State Matching" reward to fine-tune the long-range attention heads.6. Implementation on Apple Silicon: System ArchitectureThe final deliverable is the specific architecture for the Apple M4.6.1 Hardware-Software StackCompute Platform: Apple M4 (Unified Memory Architecture).Symbolic Engine (CPU): Kissat-M4, a custom build of the Kissat solver optimized for ARM64 NEON instructions. It runs on the M4's Performance Cores.Neural Engine (ANE): The BitNet Transformer runs on the ANE.Framework: Core ML for inference, Metal Performance Shaders (MPS) for any operations not supported by ANE.6.2 The "Neuro-SHA-M4" WorkflowThe system operates as two asynchronous processes communicating via Shared Memory (zero-copy).Process A: The Solver (CPU)Parses the SHA-256 instance.Runs CDCL loop (Propagate -> Conflict -> Learn).Hook: Every 5,000 conflicts, it writes the current assignment_vector to the Shared Memory Buffer.It reads the priority_vector from Shared Memory and updates VSIDS scores.Process B: The Neural Oracle (ANE)Polls the Shared Memory Buffer.Dynamic Loading: If the solver is focused on Round 30, it loads the Transformer weights relevant to the window. This "Weight Stripping" technique ensures we never load the full model if not needed.Inference: Runs the Sparse Logic Transformer (BitNet Int4).Output: Writes the priority_vector (Glue Variable probabilities) back to Shared Memory.6.3 Memory Budget Analysis (<5GB)ComponentDescriptionMemory UsageSAT Solver (Kissat)Clause DB, Watches, Trail2.5 GBNeural Oracle (BitNet)300M Params (Int4) + KV Cache0.6 GBPathfinder (ResNet)20 Layers (Int8)0.2 GBShared Memory BuffersState I/O0.1 GBOS & System OverheadmacOS Kernel, WindowServer1.2 GBTotal4.6 GBThis budget confirms the feasibility of the architecture on a base model MacBook Air (8GB RAM), leaving headroom for the OS.7. Performance & Feasibility Analysis7.1 Expected ThroughputThe M4's ANE is capable of 38 TOPS (Trillions of Operations Per Second).Our BitNet model is predominantly Integer Addition.Estimated Inference Latency: 5ms per query.Solver "Thinking" Time (5k conflicts): ~500ms.Overhead: The neural guidance adds ~1% latency overhead to the solver, which is negligible compared to the orders-of-magnitude reduction in search space size.7.2 The Speedup FactorStandard brute force explores the space randomly.The Neuro-SHA-M4 system effectively prunes the search space.If the Neural Guide can identify the correct "Glue Variables" with 90% accuracy, it reduces the branching factor from 2 (binary guess) to $\sim 1.1$.Over 64 rounds (approx 2000 bit decisions), this reduction is exponential.While we do not claim to break the full 256-bit security (which would require $2^{128}$ operations), we expect this system to solve 40-45 round reduced variants in real-time, a feat currently impossible for standard solvers.8. ConclusionThe Neuro-SHA-M4 architecture represents a pragmatic, hardware-aware approach to deep learning-based cryptanalysis. It rejects the fantasy of "black-box inversion" in favor of a scientifically grounded Solver-in-the-Loop methodology.By synthesizing Bitwise BASIN modeling, Sparse Logic Transformers, and BitNet Quantization, we have designed a system that brings state-of-the-art Neuro-Symbolic AI to consumer silicon. It transforms the preimage attack from a brute-force exhaustion of energy into a precision-guided search for structural weakness. This architecture serves as a blueprint for the future of automated cryptanalysis: not bigger clusters, but smarter, more efficient, and structurally aware algorithms.Citations