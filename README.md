<div align="center">

# ğŸ§  NEURO-SHA-M4

### *Neuro-Symbolic SHA-256 Cryptanalysis on Apple Silicon*

**Breaking SHA-256 logic on a MacBook Air M4 using < 10 GB RAM.**

[![Status](https://img.shields.io/badge/Status-Active%20Training-brightgreen)]()
[![License](https://img.shields.io/badge/License-Non--Commercial-red)](#license)
[![Platform](https://img.shields.io/badge/Platform-Apple%20M4-blue)]()
[![RAM](https://img.shields.io/badge/RAM%20Usage-0.2GB-success)]()

[Architecture](#architecture) Â· [Live Results](#live-training-results) Â· [Usage](#usage) Â· [CLI Demo](#neuro-cli) Â· [License](#license)

*Built by a Medical Student / Vibe Coder* Â· [ğ• @MohamedHz72007](https://x.com/MohamedHz72007)

</div>

---

## What is This?

Neuro-SHA-M4 is a **neuro-symbolic framework** that learns the internal logic of SHA-256 and uses that knowledge to guide a SAT solver toward preimage solutions.

Instead of trying to "invert" SHA-256 with brute force (impossible) or with a naive neural network (also impossible), we do something smarter:

> **The neural network learns *where to look*. The symbolic solver proves *what's there*.**

The model currently achieves **62%+ bit-prediction accuracy** on SHA-256 internal state transitions â€” running on a single MacBook Air M4 with **0.2GB RAM usage**.

---

## Architecture

### 1. Sparse Logic Attention â€” O(N), Not O(NÂ²)

Standard Transformers attend to *everything*. But SHA-256 is **sparse** â€” each bit only depends on a handful of neighbors defined by the `Î£â‚€`, `Î£â‚`, `Maj`, and `Ch` functions.

We hard-code the attention mask to mirror the **exact wiring diagram** of SHA-256:

```
Bit[i] attends to:
  â†’ Itself (identity)
  â†’ ROTR(2,13,22) neighbors  (Î£â‚€ wiring)
  â†’ ROTR(6,11,25) neighbors  (Î£â‚ wiring)
  â†’ Same bit across all 8 words (Vertical/Inter-word wiring)
```

**Result:** Instead of `256 Ã— 256 = 65,536` attention weights per layer, we use `256 Ã— 15 = 3,840`. That's a **17Ã— reduction** â€” enabling 24 layers on a laptop.

### 2. BitNet b1.58 â€” Ternary Weights {-1, 0, 1}

Every linear layer uses **1.58-bit quantized weights**:

| Weight | Meaning |
|--------|---------|
| `+1` | Pass this bit |
| `-1` | Invert this bit (NOT) |
| `0` | Ignore this bit |

This isn't just compression â€” it's an **inductive bias for boolean logic**. The network naturally learns AND/OR/XOR gates without floating-point drift. A 36M parameter model fits in ~25MB.

### 3. Neuro-Symbolic Bridge â€” Guiding Z3 with Heatmaps

The trained model generates **variable importance heatmaps** that tell the SAT solver which bits to assign first:

```
Process A (Solver/CPU)          Process B (Neural Oracle/MPS)
       â”‚                                â”‚
       â”‚â”€â”€ Assignment Vector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
       â”‚                                â”‚â”€â”€ Sparse Logic Transformer
       â”‚â†â”€â”€ Priority Heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                                â”‚
  CDCL Search                     BitNet Inference
  (Kissat/Z3)                      (<5ms latency)
```

The solver runs VSIDS for 5,000 conflicts, then queries the neural oracle. The oracle returns "glue variable" probabilities that refocus the solver on the structurally critical bits.

---

## Live Training Results

> Phase 1: 16-Round Logic Learning | MacBook Air M4 | 0.2GB RAM

| Step | Loss | Accuracy | RAM (GB) |
|-----:|-----:|---------:|---------:|
| 10 | 0.723 | 48.1% | 0.14 |
| 100 | 0.937 | 52.9% | 0.26 |
| 200 | 0.721 | 60.0% | 0.26 |
| 350 | 0.649 | 61.7% | 0.26 |
| 500 | 0.670 | 60.4% | 0.21 |
| 530 | 0.643 | **63.0%** | 0.09 |

The model is in active training, progressing through a 3-phase curriculum:
- **Phase 1** (Steps 0â€“1,000): 16-round SHA-256 logic
- **Phase 2** (Steps 1,000â€“5,000): 32-round extended chains
- **Phase 3** (Steps 5,000+): Full 64-round SHA-256

---

## Project Structure

```
NanoLogic/
â”œâ”€â”€ main.py                     # Training entry point (auto-resume)
â”œâ”€â”€ config.py                   # All hyperparameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ sparse_logic.py     # Sparse Logic Transformer (gradient checkpointing)
â”‚   â”‚   â”œâ”€â”€ bitnet.py           # BitNet b1.58 quantization layer
â”‚   â”‚   â”œâ”€â”€ wiring.py           # SHA-256 static wiring + trace generator
â”‚   â”‚   â””â”€â”€ pathfinder.py       # ResNet-1D distinguisher
â”‚   â”œâ”€â”€ optim/
â”‚   â”‚   â””â”€â”€ lion_galore.py      # Lion optimizer with GaLore projection
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ synthetic.py        # Lazy trace generator (zero-storage)
â”‚   â”‚   â”œâ”€â”€ curriculum.py       # 3-phase curriculum scheduler
â”‚   â”‚   â””â”€â”€ loss.py             # BCE + Hamming distance loss
â”‚   â”œâ”€â”€ solver/
â”‚   â”‚   â”œâ”€â”€ bridge.py           # Shared memory bridge (zero-copy)
â”‚   â”‚   â””â”€â”€ cnf_utils.py        # SAT encoding utilities
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ monitor.py          # MemoryGuard (10GB ceiling)
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ neuro_cli.py            # Interactive demo CLI (rich)
â”œâ”€â”€ checkpoints/                # Auto-saved every 500 steps
â”œâ”€â”€ logs/                       # CSV training logs
â”œâ”€â”€ LICENSE                     # PolyForm Noncommercial 1.0.0
â””â”€â”€ COMMERCIAL_TERMS.md         # 60/40 profit-share for commercial use
```

---

## Usage

### Requirements

```bash
pip install torch psutil rich
```

### Train

```bash
cd NanoLogic
python3 main.py
```

Training auto-resumes from the latest checkpoint in `checkpoints/`. Press `Ctrl+C` to safely stop â€” progress is always saved.

### CLI Demo

```bash
# Watch AI vs Brute Force race
python3 tools/neuro_cli.py --mode race

# Interactive hash cracker dashboard
python3 tools/neuro_cli.py --mode crack
```

---

## The Vibe Note

> *This project is built by a medical student who codes between anatomy lectures. The constraint isn't a datacenter â€” it's a MacBook Air. The optimization isn't FLOPS â€” it's RAM. The goal isn't to break SHA-256 (yet) â€” it's to prove that a purpose-built logic engine, running on consumer silicon, can learn a structure that was designed to be unlearnable.*
>
> *If you're reading this and thinking "that's impossible" â€” good. That's the point.*
>
> â€” [@MohamedHz72007](https://x.com/MohamedHz72007)

---

## License

This project is licensed under the **[PolyForm Noncommercial License 1.0.0](LICENSE)**.

- âœ… Free for research, education, and personal use
- âœ… Free to modify and redistribute (non-commercially)
- âŒ **Commercial use is strictly prohibited** without a signed agreement

**Want to use this commercially?** See **[COMMERCIAL_TERMS.md](COMMERCIAL_TERMS.md)** or contact [@MohamedHz72007](https://x.com/MohamedHz72007).

---

## Citation

If you use this work in research, please cite:

```bibtex
@software{neurosham4_2026,
  author = {Mohamed Hossam},
  title = {NanoLogic: Neuro-Symbolic SHA-256 Cryptanalysis on Apple Silicon},
  year = {2026},
  url = {https://github.com/mohammedhossammohammed/NanoLogic}
}
```
# NanoLogic
