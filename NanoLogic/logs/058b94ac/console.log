ğŸ“ Console logs mirrored to: logs/058b94ac/console.log
âœ… Device: mps
âš¡ Mixed Precision (FP16): Enabled (MPS Native)
ğŸ•¸ï¸ Model: Sparse Logic Transformer (34.9M Params)
ğŸ—ºï¸ Model: Pathfinder (ResNet)
ğŸ¦ Optimizer: LionGaLore
[SolverBridge] Connected to Shared Memory: neuro_sha_bridge
ğŸŒ‰ Bridge: Ready (Waiting for Solver...)
ğŸ›¡ï¸ Memory Guard Active: Limit 14.0GB | Poll every 10 steps
ğŸ“¦ Gradient Accumulation: 64 micro-batches (effective batch = 128)
ğŸ†• No checkpoint found for this configuration. Starting fresh.

ğŸš€ Starting Logic Learning (Deep Curriculum)...
   Phases: 8 â†’ 16 â†’ 32 â†’ 64 rounds
   Accuracy gates: 8r: 85% | 16r: 80% | 32r: 80% | 64r: 75%
   Starting from: 8 rounds (config.start_round)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Step |       Loss |   Accuracy |  Threshold |   RAM (GB) |  Phase | Rounds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§  SharedMemoryLoader: Init
   Structure: 20 buffers x 0.03 MB
   Batch: 1 traces * 8 rounds = 8 samples
   allocating persistent buffer views...

ğŸ›‘ Training Interrupted by User
ğŸ’¾ Final checkpoint saved: checkpoints/058b94ac/neuro_sha_final.pt
ğŸ’¿ SharedMemoryLoader shutdown complete.

âœ… Training Complete. 1 steps executed.
ğŸ“Š Logs: logs/058b94ac/training.log
ğŸ“ Console logs: logs/058b94ac/console.log
ğŸ“ Console logs mirrored to: logs/058b94ac/console.log
âœ… Device: mps
âš¡ Mixed Precision (FP16): Enabled (MPS Native)
ğŸ•¸ï¸ Model: Sparse Logic Transformer (34.9M Params)
ğŸ—ºï¸ Model: Pathfinder (ResNet)
ğŸ¦ Optimizer: LionGaLore
[SolverBridge] Connected to Shared Memory: neuro_sha_bridge
ğŸŒ‰ Bridge: Ready (Waiting for Solver...)
ğŸ›¡ï¸ Memory Guard Active: Limit 14.0GB | Poll every 10 steps
ğŸ“¦ Gradient Accumulation: 64 micro-batches (effective batch = 128)
ğŸ”„ Found checkpoint: checkpoints/058b94ac/neuro_sha_final.pt
   âœ… Resumed Pathfinder weights
   âœ… Resumed scheduler state | Phase: 0 | Total steps: 1

ğŸš€ Resuming Logic Learning (Deep Curriculum)...
   Phases: 8 â†’ 16 â†’ 32 â†’ 64 rounds
   Accuracy gates: 8r: 85% | 16r: 80% | 32r: 80% | 64r: 75%
   Starting from: 8 rounds (config.start_round)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Step |       Loss |   Accuracy |  Threshold |   RAM (GB) |  Phase | Rounds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§  SharedMemoryLoader: Init
   Structure: 20 buffers x 0.03 MB
   Batch: 1 traces * 8 rounds = 8 samples
   allocating persistent buffer views...
      10 |     0.7043 |    50.00% |       85% |      0.82 (0.1+0.7) |      0 |      8
      20 |     0.7072 |    48.88% |       85% |      0.80 (0.1+0.7) |      0 |      8
      30 |     0.7037 |    50.66% |       85% |      0.82 (0.1+0.7) |      0 |      8
      40 |     0.7048 |    49.42% |       85% |      0.83 (0.1+0.7) |      0 |      8
      50 |     0.7027 |    50.21% |       85% |      0.92 (0.2+0.7) |      0 |      8
      60 |     0.7033 |    49.90% |       85% |      0.81 (0.1+0.7) |      0 |      8
      70 |     0.7023 |    50.33% |       85% |      0.83 (0.1+0.7) |      0 |      8
      80 |     0.7000 |    50.89% |       85% |      0.83 (0.1+0.7) |      0 |      8
      90 |     0.7013 |    50.63% |       85% |      0.82 (0.1+0.7) |      0 |      8
     100 |     0.7001 |    51.04% |       85% |      0.93 (0.2+0.7) |      0 |      8
     110 |     0.6994 |    51.10% |       85% |      0.83 (0.1+0.7) |      0 |      8
     120 |     0.7020 |    50.12% |       85% |      0.83 (0.1+0.7) |      0 |      8
     130 |     0.7033 |    49.50% |       85% |      0.83 (0.1+0.7) |      0 |      8
     140 |     0.7033 |    49.17% |       85% |      0.83 (0.1+0.7) |      0 |      8
     150 |     0.7022 |    49.37% |       85% |      0.92 (0.2+0.7) |      0 |      8
     160 |     0.7002 |    50.02% |       85% |      0.83 (0.1+0.7) |      0 |      8
     170 |     0.6999 |    50.79% |       85% |      0.83 (0.1+0.7) |      0 |      8
     180 |     0.6994 |    50.69% |       85% |      0.82 (0.1+0.7) |      0 |      8
     190 |     0.6963 |    52.29% |       85% |      0.81 (0.1+0.7) |      0 |      8
     200 |     0.7004 |    49.69% |       85% |      0.93 (0.2+0.7) |      0 |      8
     210 |     0.6983 |    50.42% |       85% |      0.84 (0.1+0.7) |      0 |      8
     220 |     0.6959 |    51.42% |       85% |      0.84 (0.1+0.7) |      0 |      8
     230 |     0.7012 |    49.30% |       85% |      0.83 (0.1+0.7) |      0 |      8
     240 |     0.6990 |    50.68% |       85% |      0.83 (0.1+0.7) |      0 |      8
     250 |     0.6998 |    50.34% |       85% |      0.93 (0.2+0.7) |      0 |      8
     260 |     0.6981 |    50.45% |       85% |      0.82 (0.1+0.7) |      0 |      8
     270 |     0.6991 |    50.26% |       85% |      0.82 (0.1+0.7) |      0 |      8
     280 |     0.7002 |    49.43% |       85% |      0.81 (0.1+0.7) |      0 |      8
     290 |     0.7004 |    49.84% |       85% |      0.83 (0.1+0.7) |      0 |      8
     300 |     0.6972 |    50.95% |       85% |      0.93 (0.2+0.7) |      0 |      8
     310 |     0.6993 |    50.28% |       85% |      0.83 (0.1+0.7) |      0 |      8
     320 |     0.7007 |    49.31% |       85% |      0.83 (0.3+0.6) |      0 |      8
     330 |     0.6997 |    48.96% |       85% |      0.81 (0.1+0.7) |      0 |      8
     340 |     0.6961 |    50.55% |       85% |      0.83 (0.1+0.7) |      0 |      8
     350 |     0.6986 |    50.37% |       85% |      0.94 (0.2+0.7) |      0 |      8
     360 |     0.6982 |    50.49% |       85% |      0.85 (0.1+0.7) |      0 |      8
     370 |     0.6984 |    50.44% |       85% |      0.84 (0.1+0.7) |      0 |      8
     380 |     0.6984 |    50.38% |       85% |      0.84 (0.1+0.7) |      0 |      8
     390 |     0.6966 |    50.01% |       85% |      0.83 (0.1+0.7) |      0 |      8
     400 |     0.6969 |    49.91% |       85% |      0.94 (0.2+0.7) |      0 |      8
     410 |     0.6969 |    48.96% |       85% |      0.84 (0.1+0.7) |      0 |      8
     420 |     0.6979 |    49.18% |       85% |      0.85 (0.1+0.7) |      0 |      8
     430 |     0.6962 |    49.69% |       85% |      0.83 (0.1+0.7) |      0 |      8
     440 |     0.6961 |    50.20% |       85% |      0.84 (0.1+0.7) |      0 |      8
     450 |     0.6974 |    49.46% |       85% |      0.96 (0.3+0.7) |      0 |      8
     460 |     0.6957 |    49.75% |       85% |      0.84 (0.1+0.7) |      0 |      8
     470 |     0.6951 |    49.53% |       85% |      0.83 (0.1+0.7) |      0 |      8
     480 |     0.6970 |    49.06% |       85% |      0.81 (0.1+0.7) |      0 |      8
     490 |     0.6971 |    48.33% |       85% |      0.84 (0.1+0.7) |      0 |      8
     500 |     0.6967 |    49.19% |       85% |      0.91 (0.2+0.7) |      0 |      8
  ğŸ’¾ Checkpoint saved: checkpoints/058b94ac/neuro_sha_step_500.pt
     510 |     0.6970 |    48.67% |       85% |      0.81 (0.1+0.7) |      0 |      8
     520 |     0.6954 |    49.69% |       85% |      0.81 (0.1+0.7) |      0 |      8
     530 |     0.6963 |    49.17% |       85% |      0.82 (0.1+0.7) |      0 |      8
     540 |     0.6965 |    48.97% |       85% |      0.81 (0.1+0.7) |      0 |      8
     550 |     0.6944 |    50.51% |       85% |      0.88 (0.2+0.7) |      0 |      8
     560 |     0.6947 |    50.32% |       85% |      0.83 (0.1+0.7) |      0 |      8
     570 |     0.6951 |    49.70% |       85% |      0.83 (0.1+0.7) |      0 |      8
     580 |     0.6949 |    50.07% |       85% |      0.80 (0.1+0.7) |      0 |      8
