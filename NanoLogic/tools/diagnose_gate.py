import torch
import torch.nn as nn
import sys
import os
import argparse

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config import Config
from src.model.sparse_logic import SparseLogicTransformer

def diagnose_gate(checkpoint_path, force_open=False):
    print(f"ğŸ“¦ Loading checkpoint: {checkpoint_path}")
    try:
        ckpt = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        return

    # Load Model State
    state_dict = ckpt['model_state_dict']
    
    # â”€â”€ 1. Check Recurrent Gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    gate_key = 'core.gate'
    if gate_key in state_dict:
        gate_val = state_dict[gate_key].item()
        print(f"\nğŸšª Recurrent Gate Value: {gate_val:.6f}")
        
        if abs(gate_val) < 1e-6:
            print("   âš ï¸  WARNING: Gate is effectively DEAD (0.0). Recurrence is disabled.")
            if force_open:
                print("   ğŸ”§ Force-Opening Gate to 0.1...")
                state_dict[gate_key].fill_(0.1)
                print(f"   âœ… New Gate Value: {state_dict[gate_key].item():.6f}")
        else:
            print("   âœ… Gate is active (learning).")
    else:
        print(f"âŒ '{gate_key}' not found in checkpoint!")

    # â”€â”€ 2. Check BitConvSwiGLU Weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ•¸ï¸  BitConvSwiGLU Statistics:")
    
    # We look for the ConvSwiGLU in the core block
    # Prefix: core.block.conv_swiglu
    conv_prefix = "core.block.conv_swiglu"
    
    params_found = 0
    for key, tensor in state_dict.items():
        if conv_prefix in key and "weight" in key:
            mean = tensor.float().mean().item()
            std = tensor.float().std().item()
            zeros = (tensor == 0).sum().item()
            total = tensor.numel()
            sparsity = zeros / total
            
            print(f"   - {key:<40} | Mean: {mean:+.4f} | Std: {std:.4f} | Sparsity: {sparsity:.1%}")
            
            if std < 1e-6:
                print(f"     âš ï¸  WARNING: Dead weights (Std ~ 0)!")
            
            params_found += 1
            
    if params_found == 0:
        print("   âŒ No BitConvSwiGLU weights found!")

    # â”€â”€ Save if Modified â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if force_open:
        output_path = checkpoint_path.replace(".pt", "_fixed.pt")
        # Update checkpoint dictionary
        ckpt['model_state_dict'] = state_dict
        torch.save(ckpt, output_path)
        print(f"\nğŸ’¾ Saved patched checkpoint to: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Diagnose Recurrent Gate and ConvSwiGLU Weights")
    parser.add_argument("checkpoint", type=str, help="Path to checkpoint file")
    parser.add_argument("--force-open", action="store_true", help="Set gate to 0.1 if dead")
    args = parser.parse_args()
    
    if not os.path.exists(args.checkpoint):
        print(f"Error: Checkpoint '{args.checkpoint}' not found.")
        sys.exit(1)
        
    diagnose_gate(args.checkpoint, args.force_open)
